[Fragment(lineno=1,
          text='import hashlib\n'
               'import os\n'
               'from collections import defaultdict\n'
               'from concurrent.futures import ProcessPoolExecutor\n'
               '\n'
               'from tqdm import tqdm\n'
               '\n'
               '\n'
               'def md5_checksum(file_path):...\n'
               '\n'
               'class Duplicates:...\n'
               '\n'
               'def main():...\n'
               '\n'
               "if __name__ == '__main__':\n"
               '    main()\n',
          name='',
          uuid='TEST-00',
          path='find_duplicates.py'),
 Fragment(lineno=9,
          text='\n'
               '\n'
               'def md5_checksum(file_path):\n'
               '    hasher = hashlib.md5()\n'
               "    with open(file_path, 'rb') as f:\n"
               '        while True:\n'
               '            data = f.read(65536)  # Read the file in 64KB chunks\n'
               '            if not data:\n'
               '                break\n'
               '            hasher.update(data)\n'
               '    return hasher.hexdigest()\n',
          name='md5_checksum',
          uuid='TEST-01',
          path='find_duplicates.py'),
 Fragment(lineno=20,
          text='\n\nclass Duplicates:\n\n    def __init__(self, root_dir: str) -> None:...\n    def collect(self):...',
          name='Duplicates',
          uuid='TEST-02',
          path='find_duplicates.py'),
 Fragment(lineno=22,
          text='\ndef __init__(self, root_dir: str) -> None:\n    super().__init__()\n    self.root_dir = root_dir\n    self.files = []\n',
          name='Duplicates.__init__',
          uuid='TEST-03',
          path='find_duplicates.py'),
 Fragment(lineno=27,
          text='\n'
               'def collect(self):\n'
               '    files_by_size = defaultdict(list)\n'
               '    files_by_checksum = defaultdict(list)\n'
               '\n'
               '    # Group files by size\n'
               '    for dirpath, _, filenames in os.walk(self.root_dir):\n'
               '        for filename in filenames:\n'
               '            file_path = os.path.join(dirpath, filename)\n'
               '            file_size = os.path.getsize(file_path)\n'
               '            files_by_size[file_size].append(file_path)\n'
               '\n'
               '    total_size = sum(file_size * len(file_list) for file_size, file_list in files_by_size.items() if len(file_list) > 1)\n'
               '\n',
          name='Duplicates.collect',
          uuid='TEST-04',
          path='find_duplicates.py'),
 Fragment(lineno=41,
          text='    # Calculate the hash only for groups with more than one file\n'
               '    with ProcessPoolExecutor() as executor, tqdm(total=total_size, unit=\'B\', unit_scale=True, desc="Processing files") as pbar:\n'
               '        for file_size, file_list in files_by_size.items():\n'
               '            if len(file_list) > 1:\n'
               '                file_checksums = list(executor.map(md5_checksum, file_list))\n'
               '                for file_path, file_checksum in zip(file_list, file_checksums):\n'
               '                    files_by_checksum[file_checksum].append(file_path)\n'
               '                    pbar.update(file_size)\n'
               '\n'
               '    self.files.extend(file_list for file_list in files_by_checksum.values() if len(file_list) > 1)\n',
          name='Duplicates.collect',
          uuid='TEST-05',
          path='find_duplicates.py'),
 Fragment(lineno=52,
          text='\n'
               '\n'
               'def main():\n'
               '    root_dir = input("Enter the root directory to search for duplicate files: ")\n'
               '\n'
               '    if not os.path.isdir(root_dir):\n'
               '        print("Invalid directory path. Please try again.")\n'
               '        return\n'
               '\n'
               '    duplicates = Duplicates(root_dir)\n'
               '    duplicates.collect()\n'
               '    total_space_saved = 0\n'
               '\n'
               '    if not duplicates.files:\n'
               '        print("No duplicate files found.")\n'
               '        return\n'
               '\n'
               '    print("Duplicate files found:")\n'
               '    for file_list in duplicates.files:\n'
               '        print("Duplicate group:")\n'
               '        first = True\n'
               '        for file_path in file_list:\n'
               '            if not first:\n'
               '                total_space_saved += os.path.getsize(file_path)\n'
               '            else:\n'
               '                first = False\n'
               '            print(f"\\t{file_path}")\n'
               '\n'
               '    print(f"\\nTotal disk space that could be saved by deleting duplicates: {total_space_saved} bytes")\n',
          name='main',
          uuid='TEST-06',
          path='find_duplicates.py')]