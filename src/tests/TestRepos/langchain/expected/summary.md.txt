File extensions: .md

# ðŸ¦œï¸ðŸ”— LangChain
## Quick Install
## ðŸ¤” What is this?
## ðŸ“– Documentation
## ðŸš€ What can this help with?
## ðŸ’ Contributing

# Contributing to LangChain
## ðŸ—ºï¸ Guidelines
### ðŸ‘©â€ðŸ’» Contributing Code
### ðŸš©GitHub Issues
### ðŸ™‹Getting Help
## ðŸš€ Quick Start
## âœ… Common Tasks
### Code Formatting
### Linting
### Coverage
### Working with Optional Dependencies
### Testing
#### Unit Tests
#### Integration Tests
### Adding a Jupyter Notebook
## Documentation
### Contribute Documentation
### Build Documentation Locally
## ðŸ­ Release Process
### ðŸŒŸ Recognition

#### Before submitting
#### Who can review?

# Dependents

# Readme tests(draft)
## Integrations Tests
### Prepare
# add package and install it after adding:
### Prepare environment variables for local testing:
### Recording HTTP interactions with pytest-vcr
### Run some tests with coverage:

# Tracing
## Tracing Walkthrough
## Changing Sessions

# YouTube
### â›“ï¸[Official LangChain YouTube channel](https://www.youtube.com/@LangChain)â›“ï¸
### Introduction to LangChain with Harrison Chase, creator of LangChain
## Videos (sorted by views)

# Baseten
## Installation and setup
## Invoking a model

# Deployments
## [Anyscale](https://www.anyscale.com/model-serving)
## [Streamlit](https://github.com/hwchase17/langchain-streamlit-template)
## [Gradio (on Hugging Face)](https://github.com/hwchase17/langchain-gradio-template)
## [Chainlit](https://github.com/Chainlit/cookbook)
## [Beam](https://github.com/slai-labs/get-beam/tree/main/examples/langchain-question-answering)
## [Vercel](https://github.com/homanp/vercel-langchain)
## [FastAPI + Vercel](https://github.com/msoedov/langcorn)
## [Kinsta](https://github.com/kinsta/hello-world-langchain)
## [Fly.io](https://github.com/fly-apps/hello-fly-langchain)
## [Digitalocean App Platform](https://github.com/homanp/digitalocean-langchain)
## [Google Cloud Run](https://github.com/homanp/gcp-langchain)
## [SteamShip](https://github.com/steamship-core/steamship-langchain/)
## [Langchain-serve](https://github.com/jina-ai/langchain-serve)
## [BentoML](https://github.com/ssheng/BentoChain)
## [Databutton](https://databutton.com/home?new-data-app=true)

# ModelScope
## Installation and Setup
## Wrappers
### Embeddings

# Concepts
## Chain of Thought
## Action Plan Generation
## ReAct
## Self-ask
## Prompt Chaining
## Memetic Proxy
## Self Consistency
## Inception
## MemPrompt

# Quickstart Guide
## Installation
# or
## Environment Setup
## Building a Language Model Application: LLMs
## LLMs: Get predictions from a language model
## Prompt Templates: Manage prompts for LLMs
## Chains: Combine LLMs and prompts in multi-step workflows
# -> '\n\nSocktastic!'
## Agents: Dynamically Call Chains Based on User Input
# First, let's load the language model we're going to use to control the agent.
# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.
# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.
# Now let's test it out!
## Memory: Add State to Chains and Agents
## Building a Language Model Application: Chat Models
## Get Message Completions from a Chat Model
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
# -> LLMResult(generations=[[ChatGeneration(text="J'aime programmer.", generation_info=None, message=AIMessage(content="J'aime programmer.", additional_kwargs={}))], [ChatGeneration(text="J'aime l'intelligence artificielle.", generation_info=None, message=AIMessage(content="J'aime l'intelligence artificielle.", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})
# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}
## Chat Prompt Templates
# get a chat completion from the formatted messages
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
## Chains with Chat Models
# -> "J'aime programmer."
## Agents with Chat Models
# First, let's load the language model we're going to use to control the agent.
# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.
# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.
# Now let's test it out!
## Memory: Add State to Chains and Agents
# -> 'Hello! How can I assist you today?'
# -> "That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?"
# -> "Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?"

# Tutorials
### DeepLearning.AI course
### Handbook
### Tutorials
###
###
###
###
###
###

# AI21 Labs
## Installation and Setup
## Wrappers
### LLM

# Airbyte
## Installation and Setup
## Document Loader

# Aleph Alpha
## Installation and Setup
## LLM
## Text Embedding Models

# Amazon Bedrock
## Installation and Setup
## LLM
## Text Embedding Models

# AnalyticDB
### VectorStore

# Annoy
## Installation and Setup
## Vectorstore

# Anthropic
## Installation and Setup
## Chat Models

# Anyscale
## Installation and Setup
## Wrappers
### LLM

# Apify
## Overview
## Installation and Setup
## Wrappers
### Utility
### Loader

# Argilla
## Installation and Setup
## Tracking

# Arxiv
## Installation and Setup
## Document Loader
## Retriever

# AtlasDB
## Installation and Setup
## Wrappers
### VectorStore

# AwaDB
## Installation and Setup
## VectorStore

# AWS S3 Directory
## Installation and Setup
## Document Loader

# AZLyrics
## Installation and Setup
## Document Loader

# Azure Blob Storage
## Installation and Setup
## Document Loader

# Azure Cognitive Search
## Installation and Setup
## Retriever

# Azure OpenAI
## Installation and Setup
## LLM
## Text Embedding Models
## Chat Models

# Banana
## Installation and Setup
## Define your Banana Template
## Build the Banana app
# Return the results as a dictionary
## Wrappers
### LLM

# Beam
## Installation and Setup
## LLM
### Example of the Beam app
### Deploy the Beam app
### Call the Beam app

# BiliBili
## Installation and Setup
## Document Loader

# Blackboard
## Installation and Setup
## Document Loader

# Cassandra
## Installation and Setup
## Memory

# CerebriumAI
## Installation and Setup
## Wrappers
### LLM

# Chroma
## Installation and Setup
## VectorStore
## Retriever

# ClickHouse
## Installation
### Configure clickhouse vector index
## Wrappers
### VectorStore

# Cohere
## Installation and Setup
## LLM
## Text Embedding Model
## Retriever

# College Confidential
## Installation and Setup
## Document Loader

# Confluence
## Installation and Setup
## Document Loader

# C Transformers
## Installation and Setup
## Wrappers
### LLM

# Databerry
## Installation and Setup
## Retriever

# DeepInfra
## Installation and Setup
## Available Models
## Wrappers
### LLM

# Deep Lake
## Why Deep Lake?
## More Resources
## Installation and Setup
## Wrappers
### VectorStore

# Diffbot
## Installation and Setup
## Document Loader

# Discord
## Installation and Setup
## Document Loader

# Docugami
## Installation and Setup
## Document Loader

# DuckDB
## Installation and Setup
## Document Loader

# Elasticsearch
## Installation and Setup
## Retriever

# EverNote
## Installation and Setup
## Document Loader

# Facebook Chat
## Installation and Setup
## Document Loader

# Figma
## Installation and Setup
## Document Loader

# ForefrontAI
## Installation and Setup
## Wrappers
### LLM

# Git
## Installation and Setup
## Document Loader

# GitBook
## Installation and Setup
## Document Loader

# Google BigQuery
## Installation and Setup
## Document Loader

# Google Cloud Storage
## Installation and Setup
## Document Loader

# Google Drive
## Installation and Setup
## Document Loader

# Google Search
## Installation and Setup
## Wrappers
### Utility
### Tool

# Google Serper
## Setup
## Wrappers
### Utility
#### Output
### Tool

# Google Vertex AI
## Installation and Setup
## Chat Models

# GooseAI
## Installation and Setup
## Wrappers
### LLM

# GPT4All
## Installation and Setup
## Usage
### GPT4All
# Instantiate the model. Callbacks support token-wise streaming
# Generate text
# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler
# Generate text. Tokens are streamed through the callback manager.
## Model File

# Graphsignal
## Installation and Setup
## Tracing and Monitoring

# Gutenberg
## Installation and Setup
## Document Loader

# Hacker News
## Installation and Setup
## Document Loader

# Hazy Research
## Installation and Setup
## Wrappers
### LLM

# Helicone
## What is Helicone?
## Quick start
## How to enable Helicone caching
## How to use Helicone custom properties

# Hugging Face
## Installation and Setup
## Wrappers
### LLM
### Embeddings
### Tokenizer
### Datasets

# iFixit
## Installation and Setup
## Document Loader

# IMSDb
## Installation and Setup
## Document Loader

# Jina
## Installation and Setup
## Wrappers
### Embeddings

# LanceDB
## Installation and Setup
## Wrappers
### VectorStore

# LangChain Decorators âœ¨
# run it naturaly
# or
# Quick start
## Installation
## Examples
# Defining other parameters
# define global settings for all prompty (if not set - chatGPT is the current default)
#You can change the default prompt types
# Or you can just define your own ones:
## Passing a memory and/or callbacks:
# Simplified streaming
# this code example is complete and should run as it is
# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
# just an arbitrary  function to demonstrate the streaming... wil be some websockets code in the real world
# if we want to capture the stream, we need to wrap the execution into StreamingContext... 
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
# Prompt declarations
## Documenting your prompt
## Chat messages prompt
# Optional sections
# Output parsers
# this code example is complete and should run as it is
## More complex structures
# print the result nicely formatted
# Binding the prompt to an object
# More examples:

# Llama.cpp
## Installation and Setup
## Wrappers
### LLM
### Embeddings

# MediaWikiDump
## Installation and Setup
## Document Loader

# Metal
## What is Metal?
## Quick start

# Microsoft OneDrive
## Installation and Setup
## Document Loader

# Microsoft PowerPoint
## Installation and Setup
## Document Loader

# Microsoft Word
## Installation and Setup
## Document Loader

# Milvus
## Installation and Setup
## Wrappers
### VectorStore

# Modal
## Installation and Setup
## Define your Modal Functions and Webhooks
## Wrappers
### LLM

# Modern Treasury
## Installation and Setup
## Document Loader

# Momento
## Installation and Setup
## Cache
# Instantiate the Momento client
# Choose a Momento cache name of your choice
# Instantiate the LLM cache
## Memory
### Chat Message History Memory

# MyScale
## Introduction
## Installation and Setup
### Setting up envrionments
## Wrappers
### VectorStore

# NLPCloud
## Installation and Setup
## Wrappers
### LLM

# Notion DB
## Installation and Setup
## Document Loader

# Obsidian
## Installation and Setup
## Document Loader

# OpenAI
## Installation and Setup
## LLM
## Text Embedding Model
## Chat Model
## Tokenizer
## Chain
## Document Loader
## Retriever

# OpenSearch
## Installation and Setup
## Wrappers
### VectorStore

# OpenWeatherMap
## Installation and Setup
## Wrappers
### Utility
### Tool

# Petals
## Installation and Setup
## Wrappers
### LLM

# PGVector
## Installation
## Setup
## Wrappers
### VectorStore
### Usage

# Pinecone
## Installation and Setup
## Vectorstore

# PipelineAI
## Installation and Setup
## Wrappers
### LLM

# Prediction Guard
## Installation and Setup
## LLM 
### Example
#### Basic usage of the controlled or guarded LLM:
# Your Prediction Guard API key. Get one at predictionguard.com
# Define a prompt template
# With "guarding" or controlling the output of the LLM. See the 
# Prediction Guard docs (https://docs.predictionguard.com) to learn how to 
# control the output with integer, float, boolean, JSON, and other types and
# structures.
#### Basic LLM Chaining with the Prediction Guard:
# Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows
# you to access all the latest open access models (see https://docs.predictionguard.com)
# Your Prediction Guard API key. Get one at predictionguard.com

# PromptLayer
## Installation and Setup
## LLM
### Example
## Chat Model

# Psychic
## Installation and Setup
## Advantages vs Other Document Loaders

# Qdrant
## Installation and Setup
## Wrappers
### VectorStore

# Reddit
## Installation and Setup
## Document Loader

# Redis
## Installation and Setup
## Wrappers
### Cache
#### Standard Cache
#### Semantic Cache
# use any embedding provider...
### VectorStore
### Retriever
### Memory
#### Vector Store Retriever Memory
#### Chat Message History Memory

# Replicate
## Installation and Setup
## Calling a model

# Roam
## Installation and Setup
## Document Loader

# Runhouse
## Installation and Setup
## Self-hosted LLMs
## Self-hosted Embeddings

# RWKV-4
## Installation and Setup
## Usage
### RWKV
# Test the model
# Instruction:
# Input:
# Response:
# Instruction:
# Response:
## Model File
### Rwkv-4 models -> recommended VRAM

# SageMaker Endpoint
## Installation and Setup
## LLM
## Text Embedding Models

# SearxNG Search API
## Installation and Setup
### Self Hosted Instance:
## Wrappers
### Utility
### Tool

# SerpAPI
## Installation and Setup
## Wrappers
### Utility
### Tool

# Shale Protocol
## How to
### 1. Find the link to our Discord on https://shaleprotocol.com. Generate an API key through the "Shale Bot" on our Discord. No credit card is required and no free trials. It's a forever free tier with 1K limit per day per API key.
### 2. Use https://shale.live/v1 as OpenAI API drop-in replacement 
# Answer: Let's think step by step."""

# scikit-learn
## Installation and Setup
## Wrappers
### VectorStore

# Slack
## Installation and Setup
## Document Loader

# spaCy
## Installation and Setup
## Text Splitter

# Spreedly
## Installation and Setup
## Document Loader

# StochasticAI
## Installation and Setup
## Wrappers
### LLM

# Stripe
## Installation and Setup
## Document Loader

# Tair
## Installation and Setup
## Wrappers
### VectorStore

# Telegram
## Installation and Setup
## Document Loader

# Tensorflow Hub
## Installation and Setup
## Text Embedding Models

# 2Markdown
## Installation and Setup
## Document Loader

# Trello
## Installation and Setup
## Document Loader

# Twitter
## Installation and Setup
## Document Loader

# Unstructured
## Installation and Setup
## Wrappers
### Data Loaders

# Vectara
## Installation and Setup
## Usage
### VectorStore

# Vespa
## Installation and Setup
## Retriever

# Weather
## Installation and Setup
## Document Loader

# Weaviate
## Installation and Setup
## Wrappers
### VectorStore

# WhatsApp
## Installation and Setup
## Document Loader

# Wikipedia
## Installation and Setup
## Document Loader
## Retriever

# Wolfram Alpha
## Installation and Setup
## Wrappers
### Utility
### Tool

# Writer
## Installation and Setup
## Wrappers
### LLM

# Yeager.ai
## What is Yeager.ai?
## yAgents
### How to use?
### Creating and Executing Tools with yAgents

# YouTube
## Installation and Setup
## Document Loader

# Zep
## Installation and Setup
## Retriever

# Zilliz
## Installation and Setup
## Vectorstore

# Installation
## Official Releases
## Installing from source

# Title_REPLACE_ME
## Installation and Setup
## LLM
## Text Embedding Models
## Chat Models
## Document Loader

# Cloud Hosted Setup
## Installation
## Environment Setup

# Locally Hosted Setup
## Installation
## Environment Setup

# Agent Simulations
## Simulations with One Agent
## Simulations with Two Agents
## Simulations with Multiple Agents

# Interacting with APIs
## Chains
## Agents

# Autonomous Agents
## Baby AGI ([Original Repo](https://github.com/yoheinakajima/babyagi))
## AutoGPT ([Original Repo](https://github.com/Significant-Gravitas/Auto-GPT))
## MetaPrompt ([Original Repo](https://github.com/ngoodman/metaprompt))

# Chatbots

# Code Understanding
## Conversational Retriever Chain

# Extraction

# Agents
## Create Your Own Agent
### Step 1: Create Tools
### (Optional) Step 2: Modify Agent
### (Optional) Step 3: Modify Agent Executor
## Examples

# Question Answering over Docs
## Document Question Answering
## Adding in sources
## Additional Related Resources
## End-to-end examples

# Summarization

# Querying Tabular Data
## Document Loading
## Querying
### Chains
### Agents

# Agent Types
## `zero-shot-react-description`
## `react-docstore`
## `self-ask-with-search`
### `conversational-react-description`

# Getting Started
## List of Tools

# Getting Started
## What is a prompt template?
# -> I want you to act as a naming consultant for new companies.
# -> What is a good name for a company that makes colorful socks?
## Create a prompt template
# An example prompt with no input variables
# -> "Tell me a joke."
# An example prompt with one input variable
# -> "Tell me a funny joke."
# An example prompt with multiple input variables
# -> "Tell me a funny joke about chickens."
# -> ['adjective', 'content']
# -> Tell me a funny joke about chickens.
## Template formats
# Make sure jinja2 is installed before running this
# -> Tell me a funny joke about chickens.
## Validate template
## Serialize prompt template
## Pass few shot examples to a prompt template
# First, create the list of few shot examples.
# Next, we specify the template to format the examples we have provided.
# We use the `PromptTemplate` class for this.
# Finally, we create the `FewShotPromptTemplate` object.
# We can now generate a prompt using the `format` method.
# -> Give the antonym of every input
# -> 
# -> Word: happy
# -> Antonym: sad
# ->
# -> Word: tall
# -> Antonym: short
# ->
# -> Word: big
# -> Antonym: 
## Select examples for a prompt template
# These are a lot of examples of a pretend task of creating antonyms.
# We'll use the `LengthBasedExampleSelector` to select the examples.
# We can now use the `example_selector` to create a `FewShotPromptTemplate`.
# We can now generate a prompt using the `format` method.
# -> Give the antonym of every input
# ->
# -> Word: happy
# -> Antonym: sad
# ->
# -> Word: tall
# -> Antonym: short
# ->
# -> Word: energetic
# -> Antonym: lethargic
# ->
# -> Word: sunny
# -> Antonym: gloomy
# ->
# -> Word: windy
# -> Antonym: calm
# ->
# -> Word: big
# -> Antonym:
# -> Give the antonym of every input
# -> Word: happy
# -> Antonym: sad
# ->
# -> Word: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
# -> Antonym:

# How to create a custom example selector
## Implement custom example selector
## Use custom example selector
# Initialize example selector.
# Select examples
# -> array([{'foo': '2'}, {'foo': '3'}], dtype=object)
# Add new example to the set of examples
# -> [{'foo': '1'}, {'foo': '2'}, {'foo': '3'}, {'foo': '4'}]
# Select examples
# -> array([{'foo': '1'}, {'foo': '4'}], dtype=object)
